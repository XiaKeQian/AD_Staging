import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights
from AAL_APE import AAL_APE
from AAL_APE.patch_utils import compute_patch_centers
from TokenTrimmer.TokenTrimmer import TokenTrimmer
import nibabel as nib
from REAM.REAM import REAM
#åŸç‰ˆå­˜æ¡£
# ğŸ”§ è‡ªåŠ¨è¡¥é½ä½“ç§¯åˆ° patch_size çš„å€æ•°
def pad_to_divisible(x, patch_size):
    _, _, d, h, w = x.shape
    pd, ph, pw = patch_size
    dd = (pd - d % pd) if d % pd != 0 else 0
    dh = (ph - h % ph) if h % ph != 0 else 0
    dw = (pw - w % pw) if w % pw != 0 else 0
    # æ³¨æ„ï¼šF.pad çš„é¡ºåºæ˜¯ (Wå‰,Wå,Hå‰,Hå,Då‰,Då)
    x = F.pad(x, (0, dw, 0, dh, 0, dd))
    return x

# âœ… 3D Patch Embedding æ›¿ä»£ ViT ä¸­çš„ 2D patch
class PatchEmbed3D(nn.Module):
    def __init__(self, in_channels=1, embed_dim=768, patch_size=(16,16,16)):
        super().__init__()
        self.patch_size = patch_size
        self.proj = nn.Conv3d(
            in_channels, embed_dim,
            kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x):  # x: [B,1,D,H,W]
        x = pad_to_divisible(x, self.patch_size)
        x = self.proj(x)                 # [B, embed_dim, D', H', W']
        x = x.flatten(2).transpose(1,2)  # [B, N, embed_dim]
        return x

# âœ… æ•´åˆåˆ° ViT æ¨¡å‹
class ViT3D(nn.Module):
    def __init__(self, num_classes=4, patch_size=(16,16,16)):
        super().__init__()
        self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)

        self.hidden_dim = self.vit.hidden_dim  # ä¿å­˜æ–¹ä¾¿ç”¨

        # æ›¿æ¢ patch embedding ä¸º 3D patch
        self.patch_embed = PatchEmbed3D(
            in_channels=1,
            embed_dim=self.vit.hidden_dim,
            patch_size=patch_size
        )

        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_dim))
        nn.init.trunc_normal_(self.cls_token, std=0.02)

        embed_dim = self.hidden_dim
        self.aal_pe = AAL_APE.AALPositionalEmbedding(
            aal_path="./AAL_APE/AAL2.nii.gz",
            embed_dim=embed_dim,
            region_max=116  # AAL ç»“æ„æ•°é‡
        )

        self.pos_embed = None
        # åˆ é™¤åŸå§‹ ViT çš„ä½ç½®ç¼–ç é€»è¾‘
        encoder = self.vit.encoder
        del self.vit.encoder

        self.encoder_blocks = nn.Sequential(*encoder.layers)
        self.encoder_norm = encoder.ln

        # æ›¿æ¢åˆ†ç±»å¤´
        self.heads = nn.Linear(self.vit.hidden_dim, num_classes)

        # ä½ å¯ä»¥æŠŠ Mï¼ˆè¾“å‡º token æ•°ï¼‰è®¾æˆä½ æƒ³è¦çš„å€¼ï¼Œæ¯”å¦‚ 128
        self.token_learner = TokenLearner(dim=self.hidden_dim, num_output_tokens=128)

        # REAMï¼ˆæ¨¡å—3ï¼‰
        self.ream = REAM(embed_dim=self.hidden_dim, num_regions=116)

        # def forward(self, x):  # x: [B,1,D,H,W]
    #     return self.vit(x)
    def forward(self, x, mri_affine, original_shape):  # æ–°å¢ä¸¤ä¸ªå‚æ•°  # x: [B, 1, D, H, W]
        x = self.patch_embed(x)  # 3D patch embedding â†’ [B, N, C]
        B, N, C = x.shape

        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # [B, 1, C]
        x = torch.cat((cls_token, x), dim=1)  # æ·»åŠ åˆ†ç±» token â†’ [B, N+1, C]


        # # ğŸ”§ åŠ¨æ€ç”Ÿæˆ position embedding
        # if (self.pos_embed is None) or (self.pos_embed.size(1) != x.size(1)):
        #     self.pos_embed = nn.Parameter(
        #         torch.zeros(1, x.size(1), C, device=x.device)
        #     )
        #     nn.init.trunc_normal_(self.pos_embed, std=0.02)
        #
        # x = x + self.pos_embed[:, :x.size(1)]  # ğŸ”§ é˜²æ­¢ patch æ•°ä¸åŒæ—¶æŠ¥é”™


        # =============== ğŸ”¥ åŠ å…¥ç»“æ„ä½ç½®ç¼–ç  ===============
        # âœ… Padding å·²çŸ¥ï¼š
        # è¾“å…¥ä½“ç§¯ shape = (197, 233, 189)
        # patch size = (16, 16, 16)
        # padding = (D:11, H:7, W:3)
        patch_centers = compute_patch_centers(
            img_shape=original_shape,
            patch_size=self.patch_embed.patch_size,
            padding=(11, 7, 3)
        )  # [N, 3] voxel ä¸­å¿ƒç‚¹

        patch_centers = patch_centers.unsqueeze(0).expand(B, -1, -1).to(x.device)  # [B, N, 3]

        region_pe = self.aal_pe(patch_centers, mri_affine)  # [B, N, C]
        # âœ… ä¸º cls_token æ·»åŠ ç»“æ„ä½ç½®ç¼–ç ï¼ˆå…¨ 0 å‘é‡ï¼‰
        cls_pe = torch.zeros((B, 1, C), device=x.device)  # [B, 1, C]
        region_pe = torch.cat([cls_pe, region_pe], dim=1)  # [B, N+1, C]


        x = x + region_pe
        # # æ’å…¥æ¨¡å—2æ—¶è¿™é‡Œè¢«ä¿®æ”¹
        # # æ‹†å‡º cls_token å’Œ patch_tokens
        # cls_token = x[:, :1, :]  # [B,1,C]
        # patch_tokens = x[:, 1:, :]  # [B,N,C]
        #
        # # â‘  ç”¨ TokenLearner åš token ç­›é€‰ï¼å‹ç¼©
        # #    learned_tokens: [B, M, C]
        # learned_tokens = self.token_learner(patch_tokens)
        #
        # # â‘¡ æŠŠ cls_token æ‹¼å› learned_tokens
        # x = torch.cat([cls_token, learned_tokens], dim=1)  # [B, 1+M, C]
        #
        # # â‘¢ ç»§ç»­é€å…¥ Transformer Encoder
        x = self.encoder_blocks(x)
        x = self.encoder_norm(x)
        out = self.heads(x[:, 0])
        return out

        # ================================================

        # x = self.encoder_blocks(x)  # âœ… è‡ªå·±è°ƒç”¨ block
        # x = self.encoder_norm(x)
        # x = self.heads(x[:, 0])  # å– [CLS] token
        # return x
